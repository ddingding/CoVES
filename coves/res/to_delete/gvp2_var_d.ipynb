{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c514e63d-a58a-4958-94e9-968b408cbaa9",
   "metadata": {},
   "source": [
    "# making a gvp training with larger distance of atoms to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d20d73-b060-4185-aab4-5f54aed5865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gvp\n",
    "from atom3d.datasets import LMDBDataset\n",
    "import torch_geometric\n",
    "from functools import partial\n",
    "import gvp.atom3d\n",
    "import torch.nn as nn\n",
    "import tqdm, torch, time, os\n",
    "import numpy as np\n",
    "from atom3d.util import metrics\n",
    "import sklearn.metrics as sk_metrics\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats\n",
    "print = partial(print, flush=True)\n",
    "\n",
    "models_dir = 'models'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_id = float(time.time())\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111ff76-f887-4aa9-816f-a209f596e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "741a5768-d868-4602-a752-03055d802c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tqdm, random\n",
    "import torch, math\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_cluster\n",
    "\n",
    "\n",
    "\n",
    "from atom3d.datasets import LMDBDataset\n",
    "#import atom3d.datasets.ppi.neighbors as nb\n",
    "from torch.utils.data import IterableDataset\n",
    "import atom3d\n",
    "\n",
    "\n",
    "_NUM_ATOM_TYPES = 9\n",
    "_element_mapping = lambda x: {\n",
    "    'H' : 0,\n",
    "    'C' : 1,\n",
    "    'N' : 2,\n",
    "    'O' : 3,\n",
    "    'F' : 4,\n",
    "    'S' : 5,\n",
    "    'Cl': 6, 'CL': 6,\n",
    "    'P' : 7\n",
    "}.get(x, 8)\n",
    "_amino_acids = lambda x: {\n",
    "    'ALA': 0,\n",
    "    'ARG': 1,\n",
    "    'ASN': 2,\n",
    "    'ASP': 3,\n",
    "    'CYS': 4,\n",
    "    'GLU': 5,\n",
    "    'GLN': 6,\n",
    "    'GLY': 7,\n",
    "    'HIS': 8,\n",
    "    'ILE': 9,\n",
    "    'LEU': 10,\n",
    "    'LYS': 11,\n",
    "    'MET': 12,\n",
    "    'PHE': 13,\n",
    "    'PRO': 14,\n",
    "    'SER': 15,\n",
    "    'THR': 16,\n",
    "    'TRP': 17,\n",
    "    'TYR': 18,\n",
    "    'VAL': 19\n",
    "}.get(x, 20)\n",
    "_DEFAULT_V_DIM = (100, 16)\n",
    "_DEFAULT_E_DIM = (32, 1)\n",
    "\n",
    "\n",
    "def _normalize(tensor, dim=-1):\n",
    "    '''\n",
    "    Normalizes a `torch.Tensor` along dimension `dim` without `nan`s.\n",
    "    '''\n",
    "    return torch.nan_to_num(\n",
    "        torch.div(tensor, torch.norm(tensor, dim=dim, keepdim=True)))\n",
    "\n",
    "\n",
    "def _rbf(D, D_min=0., D_max=20., D_count=16, device='cpu'):\n",
    "    '''\n",
    "    From https://github.com/jingraham/neurips19-graph-protein-design\n",
    "    \n",
    "    Returns an RBF embedding of `torch.Tensor` `D` along a new axis=-1.\n",
    "    That is, if `D` has shape [...dims], then the returned tensor will have\n",
    "    shape [...dims, D_count].\n",
    "    '''\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "    D_mu = D_mu.view([1, -1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "\n",
    "    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)\n",
    "    return RBF\n",
    "\n",
    "def _edge_features(coords, edge_index, D_max=4.5, num_rbf=16, device='cpu'):\n",
    "    \n",
    "    E_vectors = coords[edge_index[0]] - coords[edge_index[1]]\n",
    "    rbf = _rbf(E_vectors.norm(dim=-1), \n",
    "               D_max=D_max, D_count=num_rbf, device=device)\n",
    "\n",
    "    edge_s = rbf\n",
    "    edge_v = _normalize(E_vectors).unsqueeze(-2)\n",
    "\n",
    "    edge_s, edge_v = map(torch.nan_to_num,\n",
    "            (edge_s, edge_v))\n",
    "\n",
    "    return edge_s, edge_v\n",
    "\n",
    "class BaseTransform:\n",
    "    '''\n",
    "    Implementation of an ATOM3D Transform which featurizes the atomic\n",
    "    coordinates in an ATOM3D dataframes into `torch_geometric.data.Data`\n",
    "    graphs. This class should not be used directly; instead, use the\n",
    "    task-specific transforms, which all extend BaseTransform. Node\n",
    "    and edge features are as described in the EGNN manuscript.\n",
    "    \n",
    "    Returned graphs have the following attributes:\n",
    "    -x          atomic coordinates, shape [n_nodes, 3]\n",
    "    -atoms      numeric encoding of atomic identity, shape [n_nodes]\n",
    "    -edge_index edge indices, shape [2, n_edges]\n",
    "    -edge_s     edge scalar features, shape [n_edges, 16]\n",
    "    -edge_v     edge scalar features, shape [n_edges, 1, 3]\n",
    "    \n",
    "    Subclasses of BaseTransform will produce graphs with additional \n",
    "    attributes for the tasks-specific training labels, in addition \n",
    "    to the above.\n",
    "    \n",
    "    All subclasses of BaseTransform directly inherit the BaseTransform\n",
    "    constructor.\n",
    "    \n",
    "    :param edge_cutoff: distance cutoff to use when drawing edges\n",
    "    :param num_rbf: number of radial bases to encode the distance on each edge\n",
    "    :device: if \"cuda\", will do preprocessing on the GPU\n",
    "    '''\n",
    "    def __init__(self, edge_cutoff=4.5, num_rbf=16, device='cpu'):\n",
    "        self.edge_cutoff = edge_cutoff\n",
    "        self.num_rbf = num_rbf\n",
    "        self.device = device\n",
    "            \n",
    "    def __call__(self, df):\n",
    "        '''\n",
    "        :param df: `pandas.DataFrame` of atomic coordinates\n",
    "                    in the ATOM3D format\n",
    "        \n",
    "        :return: `torch_geometric.data.Data` structure graph\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            coords = torch.as_tensor(df[['x', 'y', 'z']].to_numpy(),\n",
    "                                     dtype=torch.float32, device=self.device)\n",
    "            atoms = torch.as_tensor(list(map(_element_mapping, df.element)),\n",
    "                                            dtype=torch.long, device=self.device)\n",
    "\n",
    "            edge_index = torch_cluster.radius_graph(coords, r=self.edge_cutoff)\n",
    "\n",
    "            edge_s, edge_v = _edge_features(coords, edge_index, \n",
    "                                D_max=self.edge_cutoff, num_rbf=self.num_rbf, device=self.device)\n",
    "\n",
    "            return torch_geometric.data.Data(x=coords, atoms=atoms,\n",
    "                        edge_index=edge_index, edge_s=edge_s, edge_v=edge_v)\n",
    "\n",
    "class myRESDataset(IterableDataset):\n",
    "    '''\n",
    "    A `torch.utils.data.IterableDataset` wrapper around a\n",
    "    ATOM3D RES dataset.\n",
    "    \n",
    "    On each iteration, returns a `torch_geometric.data.Data`\n",
    "    graph with the attribute `label` encoding the masked residue\n",
    "    identity, `ca_idx` for the node index of the alpha carbon, \n",
    "    and all structural attributes as described in BaseTransform.\n",
    "    \n",
    "    Excludes hydrogen atoms.\n",
    "    \n",
    "    :param lmdb_dataset: path to ATOM3D dataset\n",
    "    :param split_path: path to the ATOM3D split file\n",
    "    '''\n",
    "    def __init__(self, lmdb_dataset, d_cutoff, split_path):\n",
    "        self.dataset = LMDBDataset(lmdb_dataset)\n",
    "        self.idx = list(map(int, open(split_path).read().split()))\n",
    "        self.transform = BaseTransform(edge_cutoff = d_cutoff)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            gen = self._dataset_generator(list(range(len(self.idx))), \n",
    "                      shuffle=True)\n",
    "        else:  \n",
    "            per_worker = int(math.ceil(len(self.idx) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, len(self.idx))\n",
    "            gen = self._dataset_generator(list(range(len(self.idx)))[iter_start:iter_end],\n",
    "                      shuffle=True)\n",
    "        return gen\n",
    "    \n",
    "    def _dataset_generator(self, indices, shuffle=True):\n",
    "        if shuffle: random.shuffle(indices)\n",
    "        with torch.no_grad(): # Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
    "            my_c=0\n",
    "            for idx in indices:\n",
    "                data = self.dataset[self.idx[idx]]\n",
    "                atoms = data['atoms']\n",
    "                for sub in data['labels'].itertuples():\n",
    "                    _, num, aa = sub.subunit.split('_')\n",
    "                    num, aa = int(num), _amino_acids(aa)\n",
    "                    if aa == 20: continue\n",
    "                    my_atoms = atoms.iloc[data['subunit_indices'][sub.Index]].reset_index(drop=True)\n",
    "                    #if my_c <3:\n",
    "                    #    print(set(my_atoms.element))\n",
    "                    #    my_c+=1\n",
    "                    ca_idx = np.where((my_atoms.residue == num) & (my_atoms.name == 'CA'))[0]\n",
    "                    if len(ca_idx) != 1: continue\n",
    "                        \n",
    "                    with torch.no_grad():\n",
    "                        graph = self.transform(my_atoms)\n",
    "                        graph.label = aa\n",
    "                        graph.ca_idx = int(ca_idx)\n",
    "                        yield graph\n",
    "\n",
    "def get_datasets(task, d_cutoff):\n",
    "    data_path = {\n",
    "        'RES' : '/n/groups/marks/users/david/res/atom3d_data/raw/RES/data/',\n",
    "    }[task]\n",
    "\n",
    "    if task == 'RES':\n",
    "        split_path = '/n/groups/marks/users/david/res/atom3d_data/split-by-cath-topology/indices/'\n",
    "        dataset = partial(myRESDataset, data_path, d_cutoff)        \n",
    "        trainset = dataset(split_path=split_path+'train_indices.txt')\n",
    "        valset = dataset(split_path=split_path+'val_indices.txt')\n",
    "        testset = dataset(split_path=split_path+'test_indices.txt')\n",
    "\n",
    "\n",
    "    return trainset, valset, testset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4f84e9c-4d68-43ac-b0c8-2547917dd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cutoff = 4.5\n",
    "datasets = get_datasets('RES', d_cutoff)\n",
    "\n",
    "\n",
    "batch_size = 8 # control memory of model as long as you can fit a batch size of 1, you can do gradient accumultion to simulate batch size.\n",
    "num_workers = 4\n",
    "dataloader = partial(torch_geometric.data.DataLoader, \n",
    "                    num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "trainset, valset, testset = map(dataloader, datasets)   # apply dataloader to all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b0baf69-c030-44cf-884a-2d69d3daf28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3808, 3])\n",
      "torch.Size([2, 57032])\n",
      "torch.Size([3120, 3])\n",
      "torch.Size([2, 46486])\n",
      "torch.Size([3396, 3])\n",
      "torch.Size([2, 52170])\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for b in testset:\n",
    "    print(b.x.shape)\n",
    "    print(b.edge_index.shape)\n",
    "\n",
    "    c+=1\n",
    "    if c>2:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "274967a7-30b6-4fcb-9cc7-bce202470816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4181, 3])\n",
      "torch.Size([2, 0])\n",
      "torch.Size([4130, 3])\n",
      "torch.Size([2, 0])\n",
      "torch.Size([3808, 3])\n",
      "torch.Size([2, 0])\n"
     ]
    }
   ],
   "source": [
    "d_cutoff = \n",
    "datasets = get_datasets('RES', d_cutoff)\n",
    "\n",
    "\n",
    "batch_size = 8 # control memory of model as long as you can fit a batch size of 1, you can do gradient accumultion to simulate batch size.\n",
    "num_workers = 4\n",
    "dataloader = partial(torch_geometric.data.DataLoader, \n",
    "                    num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "trainset2, valset2, testset2 = map(dataloader, datasets)   # apply dataloader to all the datasets\n",
    "\n",
    "c=0\n",
    "for b in testset2:\n",
    "    print(b.x.shape)\n",
    "    print(b.edge_index.shape)\n",
    "\n",
    "    c+=1\n",
    "    if c>2:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0d233-d434-4e27-ae28-d63ff7c64fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-1.9-gpu)",
   "language": "python",
   "name": "pytorch-1.9-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8265cf3-d5a7-4a75-84ba-b7801f4c4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use source activate /n/groups/marks/software/anaconda_o2/envs/dd_torch\n",
    "\n",
    "import esm\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b84d71-1e82-4e5d-a579-6e255b9c66ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(esm.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd3602a-c40c-4133-bf64-c93d1cc10b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/pretrained.py:175: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  \"Regression weights not found, predicting contacts will not produce correct results.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading structure in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/biotite/structure/io/pdbx/convert.py:287: UserWarning: Attribute 'auth_seq_id' not found within 'atom_site' category. The fallback attribute 'label_seq_id' will be used instead\n",
      "  UserWarning\n",
      "/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/biotite/structure/io/pdbx/convert.py:287: UserWarning: Attribute 'auth_comp_id' not found within 'atom_site' category. The fallback attribute 'label_comp_id' will be used instead\n",
      "  UserWarning\n",
      "/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/biotite/structure/io/pdbx/convert.py:287: UserWarning: Attribute 'auth_atom_id' not found within 'atom_site' category. The fallback attribute 'label_atom_id' will be used instead\n",
      "  UserWarning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "\n",
    "# to get rid of random dropout\n",
    "model= model.eval()\n",
    "\n",
    "\n",
    "\n",
    "print('reading structure in')\n",
    "# read structure in \n",
    "cifpath = '/n/groups/marks/users/david/esm_if/data/bio_all_rm_non_chain.cif' # .pdb format is also acceptable\n",
    "coords, seqs = esm.inverse_folding.multichain_util.load_complex_coords(\n",
    "    cifpath, \n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64047603-91d6-45da-96d4-342e346d317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVVRLRVAVTPEQAARMRELVEAGWYATESEIVREAVFRWELEERLRRRDVRRLRELWEEGRRSGEPRPVDFGELRERAEEALRG\n",
      "gen 1 seq took 14.74303936958313 seconds\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e69403e-43ae-4a6e-acdf-f52e76e4478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MITERLSVRVTPEQARVMDELVAAGRYATRSEIVREAVFRWRLAQERYRRDVRTLRRLWEEGRASGEPRPVDFAELREEARARLG\n",
      "gen 1 seq took 14.071446180343628 seconds\n"
     ]
    }
   ],
   "source": [
    "# fix positions in esm_if\n",
    "\n",
    "def _concatenate_coords(coords, target_chain_id, padding_length=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C\n",
    "            coordinates representing the backbone of each chain\n",
    "        target_chain_id: The chain id to sample sequences for\n",
    "        padding_length: Length of padding between concatenated chains\n",
    "    Returns:\n",
    "        Tuple (coords, seq)\n",
    "            - coords is an L x 3 x 3 array for N, CA, C coordinates, a\n",
    "              concatenation of the chains with padding in between\n",
    "            - seq is the extracted sequence, with padding tokens inserted\n",
    "              between the concatenated chains\n",
    "    \"\"\"\n",
    "    pad_coords = np.full((padding_length, 3, 3), np.nan, dtype=np.float32)\n",
    "    # For best performance, put the target chain first in concatenation.\n",
    "    coords_list = [coords[target_chain_id]]\n",
    "    for chain_id in coords:\n",
    "        if chain_id == target_chain_id:\n",
    "            continue\n",
    "        coords_list.append(pad_coords)\n",
    "        coords_list.append(coords[chain_id])\n",
    "    coords_concatenated = np.concatenate(coords_list, axis=0)\n",
    "    return coords_concatenated\n",
    "\n",
    "def sample_sequence_in_complex(model, coords, target_chain_id, temperature=1.,\n",
    "        padding_length=10, mask_pattern = None):\n",
    "    \"\"\"\n",
    "    Samples sequence for one chain in a complex.\n",
    "    Args:\n",
    "        model: An instance of the GVPTransformer model\n",
    "        coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C\n",
    "            coordinates representing the backbone of each chain\n",
    "        target_chain_id: The chain id to sample sequences for\n",
    "        padding_length: padding length in between chains\n",
    "    Returns:\n",
    "        Sampled sequence for the target chain\n",
    "    \"\"\"\n",
    "    target_chain_len = coords[target_chain_id].shape[0]\n",
    "    all_coords = _concatenate_coords(coords, target_chain_id) # puts the target chain first\n",
    "\n",
    "    # Supply padding tokens for other chains to avoid unused sampling for speed\n",
    "    padding_pattern = ['<pad>'] * all_coords.shape[0]\n",
    "    for i in range(target_chain_len):\n",
    "        padding_pattern[i] = '<mask>'\n",
    "    \n",
    "    if mask_pattern != None:\n",
    "        # make sure the supplied mask pattern is the correct length for the sequence\n",
    "\n",
    "        assert len(mask_pattern) == target_chain_len\n",
    "        for i in range(len(mask_pattern)):\n",
    "            padding_pattern[i] = mask_pattern[i]\n",
    "\n",
    "        \n",
    "    sampled = model.sample(all_coords, partial_seq=padding_pattern,\n",
    "            temperature=temperature)\n",
    "    sampled = sampled[:target_chain_len]\n",
    "    return sampled\n",
    "\n",
    "# try really low temperature\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    sampled_seq = sample_sequence_in_complex(\n",
    "        model,\n",
    "        coords,\n",
    "        'C',\n",
    "        temperature = 1e-30\n",
    "    )\n",
    "    print(sampled_seq)\n",
    "end = time.time()\n",
    "\n",
    "print('gen 1 seq took {} seconds'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12ed1d2-029c-47b1-b022-4517ed821941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351de2b6-c4b9-402e-a9af-9feea29684c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1545e25b-20c3-48ff-a44b-6ea43782fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANVEKNSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "gen 1 seq took 12.993638753890991 seconds\n"
     ]
    }
   ],
   "source": [
    "# set the mask to wildtype \n",
    "mask_list_chC = list(seqs['C'])\n",
    "mask_list_chC[5] = '<mask>'\n",
    "# try really low temperature\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    sampled_seq = sample_sequence_in_complex(\n",
    "        model,\n",
    "        coords,\n",
    "        'C',\n",
    "        temperature = 10, \n",
    "        mask_pattern = mask_list_chC\n",
    "    )\n",
    "    print(sampled_seq)\n",
    "    print(seqs['C'])\n",
    "end = time.time()\n",
    "\n",
    "print('gen 1 seq took {} seconds'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ad259ce-e9b0-47ac-89ec-41aeddd44674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(wt_seq, mut_str_m1, offset = 1):\n",
    "    # makes a list of positions to mask based on a mustring\n",
    "    mask_list_chC = list(wt_seq)\n",
    "\n",
    "    for m in mut_str_m1.split(':'):\n",
    "        wt_aa = m[0]\n",
    "        aa_pos = int(m[1:-1])\n",
    "        aa_pos_off = aa_pos - offset\n",
    "        \n",
    "        assert mask_list_chC[aa_pos_off] == wt_aa\n",
    "        mask_list_chC[aa_pos_off] = '<mask>'\n",
    "    return mask_list_chC\n",
    "\n",
    "ch_c_mask_3_pos = generate_mask(\n",
    "    seqs['C'], \n",
    "    'D61A:K64A:E80A',\n",
    "    offset = 2)\n",
    "ch_c_mask_4_pos = generate_mask(\n",
    "    seqs['C'], \n",
    "    'L59A:W60L:D61A:K64L',\n",
    "    offset = 2)\n",
    "\n",
    "ch_c_mask_10_pos = generate_mask(\n",
    "    seqs['C'], \n",
    "    'L48L:D52D:I53I:R55R:L56L:F74F:R78R:E80E:A81A:R82R',\n",
    "    offset = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed538e35-d532-4678-bbca-80e6ac193d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDIRRRRQLWDEGKASGRPEPVDYDALRKKAEQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    sampled_seq = sample_sequence_in_complex(\n",
    "        model,\n",
    "        coords,\n",
    "        'C',\n",
    "        temperature = 1, \n",
    "        mask_pattern = ch_c_mask_10_pos\n",
    "    )\n",
    "    print(sampled_seq)\n",
    "    print(seqs['C'])\n",
    "    print(hamming(sampled_seq, seqs['C']))\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "905e1cf9-3c9c-4416-bcd4-ef94be882f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(str1, str2):\n",
    "    assert len(str1) == len(str2)\n",
    "    return sum(c1 != c2 for c1, c2 in zip(str1, str2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "262dfcaa-bada-40d3-aee7-04a893c8da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 0.001, hamming dist 6\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 0.01, hamming dist 6\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 0.1, hamming dist 6\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDEERDRRQLWDEGKASGRPEPVDEDALRKKKKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 1, hamming dist 9\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDETRSKRQLWDEGKASGRPEPVDSDALHKLDSQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 10, hamming dist 10\n"
     ]
    }
   ],
   "source": [
    "# figuring out how many mutations roughly for each temperature\n",
    "for t in [1e-3, 1e-2, 1e-1,1,10]:\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        sampled_seq = sample_sequence_in_complex(\n",
    "            model,\n",
    "            coords,\n",
    "            'C',\n",
    "            temperature = t, \n",
    "            mask_pattern = ch_c_mask_10_pos\n",
    "        )\n",
    "        print(sampled_seq)\n",
    "        print(seqs['C'])\n",
    "        print('temperature {}, hamming dist {}'.format(t,hamming(sampled_seq, seqs['C'])))\n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa2789f2-9bbe-47ef-99e6-5d3792c7000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 1e-06, hamming dist 6\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 1e-06, hamming dist 6\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 1e-05, hamming dist 6\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 1e-05, hamming dist 6\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 0.0001, hamming dist 6\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 0.0001, hamming dist 6\n"
     ]
    }
   ],
   "source": [
    "for t in [1e-6, 1e-5, 1e-4]:\n",
    "    for i in range(2):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            sampled_seq = sample_sequence_in_complex(\n",
    "                model,\n",
    "                coords,\n",
    "                'C',\n",
    "                temperature = t, \n",
    "                mask_pattern = ch_c_mask_10_pos\n",
    "            )\n",
    "            print(sampled_seq)\n",
    "            print(seqs['C'])\n",
    "            print('temperature {}, hamming dist {}'.format(t,hamming(sampled_seq, seqs['C'])))\n",
    "        end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfb5acc2-6216-4718-82db-99650d879994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDYDALRKKAKQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLT\n",
      "temperature 1e-30, hamming dist 6\n"
     ]
    }
   ],
   "source": [
    "for t in [1e-30]:\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        sampled_seq = sample_sequence_in_complex(\n",
    "            model,\n",
    "            coords,\n",
    "            'C',\n",
    "            temperature = t, \n",
    "            mask_pattern = ch_c_mask_10_pos\n",
    "        )\n",
    "        print(sampled_seq)\n",
    "        print(seqs['C'])\n",
    "        print('temperature {}, hamming dist {}'.format(t,hamming(sampled_seq, seqs['C'])))\n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8765965a-a813-4728-9960-56d8d58d5462",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4b2a0f129da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mmask_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch_c_mask_10_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         )\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a97f360bc7e4>\u001b[0m in \u001b[0;36msample_sequence_in_complex\u001b[0;34m(model, coords, target_chain_id, temperature, padding_length, mask_pattern)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     sampled = model.sample(all_coords, partial_seq=padding_pattern,\n\u001b[0;32m---> 58\u001b[0;31m             temperature=temperature)\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0msampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtarget_chain_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_transformer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, coords, partial_seq, temperature, confidence)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0msampled_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0msampled_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "for t in [1e-40]:\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        sampled_seq = sample_sequence_in_complex(\n",
    "            model,\n",
    "            coords,\n",
    "            'C',\n",
    "            temperature = t, \n",
    "            mask_pattern = ch_c_mask_10_pos\n",
    "        )\n",
    "        print(sampled_seq)\n",
    "        print(seqs['C'])\n",
    "        print('temperature {}, hamming dist {}'.format(t,hamming(sampled_seq, seqs['C'])))\n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403195b0-b8c6-43cc-9fe9-6ed94a26590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "writing to /n/groups/marks/users/david/esm_if/data/gen_seqs/esm_t1_pos10_n100_gen_seq.csv\n",
      "loading model in\n",
      "reading structure in\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDERRRRQLWDEGKASGRPEPVDFDALRKRAAQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDKRRRRQLWDEGKASGRPEPVDFDALRKKAAQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDETRKRRQLWDEGKASGRPEPVDFDALRKEAIQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDDQRRERQLWDEGKASGRPEPVDYDALEKRARQKLT\n",
      "ANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKREERHDEERRERQLWDEGKASGRPEPVDRDALRKRARQKLT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b56155979db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0mmask_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch_c_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             )\n\u001b[1;32m    150\u001b[0m             \u001b[0msampled_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2b56155979db>\u001b[0m in \u001b[0;36msample_sequence_in_complex\u001b[0;34m(model, coords, target_chain_id, temperature, padding_length, mask_pattern)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     sampled = model.sample(all_coords, partial_seq=padding_pattern,\n\u001b[0;32m--> 136\u001b[0;31m             temperature=temperature)\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0msampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtarget_chain_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_transformer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, coords, partial_seq, temperature, confidence)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Run encoder only once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mencoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Decode one token at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_transformer_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, coords, encoder_padding_mask, confidence, return_all_hiddens)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \"\"\"\n\u001b[1;32m    154\u001b[0m         x, encoder_embedding = self.forward_embedding(coords,\n\u001b[0;32m--> 155\u001b[0;31m                 encoder_padding_mask, confidence)\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;31m# account for padding while computing the representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mencoder_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_transformer_encoder.py\u001b[0m in \u001b[0;36mforward_embedding\u001b[0;34m(self, coords, padding_mask, confidence)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# GVP encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         gvp_out_scalars, gvp_out_vectors = self.gvp_encoder(coords,\n\u001b[0;32m---> 92\u001b[0;31m                 coord_mask, padding_mask, confidence)\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rotation_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Rotate to local rotation frame for rotation-invariance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, coords, coord_mask, padding_mask, confidence)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             node_embeddings, edge_embeddings = layer(node_embeddings,\n\u001b[0;32m---> 53\u001b[0;31m                     edge_index, edge_embeddings)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mnode_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munflatten_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, autoregressive_x, node_mask)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    303\u001b[0m         message = self.propagate(edge_index, \n\u001b[1;32m    304\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     edge_attr=edge_attr)\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                         \u001b[0mmsg_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_modules.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, s_i, v_i, s_j, v_j, edge_attr)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mv_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple_cat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/esm/inverse_folding/gvp_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mvn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_norm_no_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_act\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/groups/marks/software/anaconda_o2/envs/dd_torch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import esm\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "models_dir = 'models'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_id = float(time.time())\n",
    "print(device)\n",
    "\n",
    "\n",
    "# batch score\n",
    "datapath = '/n/groups/marks/users/david/esm_if/data/gen_seqs/'\n",
    "num_pos_mut = 10#int(sys.argv[1])\n",
    "t = 1 #float(sys.argv[2]) # temperature to sample at\n",
    "n_seqs = 100\n",
    "\n",
    "pout = datapath + 'esm_t{}_pos{}_n{}_gen_seq.csv'.format(t, num_pos_mut, n_seqs)\n",
    "print('writing to {}'.format(pout))\n",
    "\n",
    "\n",
    "print('loading model in')\n",
    "# load model\n",
    "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "# to get rid of random dropout\n",
    "model= model.eval()\n",
    "\n",
    "print('reading structure in')\n",
    "# read structure in \n",
    "cifpath = '/n/groups/marks/users/david/esm_if/data/bio_all_rm_non_chain.cif' # .pdb format is also acceptable\n",
    "coords, seqs = esm.inverse_folding.multichain_util.load_complex_coords(\n",
    "    cifpath, \n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    ")\n",
    "\n",
    "# getting the right sequence position mask\n",
    "def generate_mask(wt_seq, mut_str_m1, offset = 1):\n",
    "    # makes a list of positions to mask based on a mustring\n",
    "    mask_list_chC = list(wt_seq)\n",
    "\n",
    "    for m in mut_str_m1.split(':'):\n",
    "        wt_aa = m[0]\n",
    "        aa_pos = int(m[1:-1])\n",
    "        aa_pos_off = aa_pos - offset\n",
    "        \n",
    "        assert mask_list_chC[aa_pos_off] == wt_aa\n",
    "        mask_list_chC[aa_pos_off] = '<mask>'\n",
    "    return mask_list_chC\n",
    "\n",
    "ch_c_mask_3_pos = generate_mask(\n",
    "    seqs['C'], \n",
    "    'D61A:K64A:E80A',\n",
    "    offset = 2)\n",
    "ch_c_mask_4_pos = generate_mask(\n",
    "    seqs['C'], \n",
    "    'L59A:W60L:D61A:K64L',\n",
    "    offset = 2)\n",
    "\n",
    "ch_c_mask_10_pos = generate_mask(\n",
    "    seqs['C'], \n",
    "    'L48L:D52D:I53I:R55R:L56L:F74F:R78R:E80E:A81A:R82R',\n",
    "    offset = 2)\n",
    "\n",
    "if num_pos_mut == 3:\n",
    "    ch_c_mask = ch_c_mask_3_pos\n",
    "elif num_pos_mut == 4:\n",
    "    ch_c_mask = ch_c_mask_4_pos\n",
    "elif num_pos_mut == 10:\n",
    "    ch_c_mask = ch_c_mask_10_pos\n",
    "else:\n",
    "    print('wrong input given for num positions to mutate')\n",
    "\n",
    "    \n",
    "##############################################\n",
    "##### sampling ###############################\n",
    "def _concatenate_coords(coords, target_chain_id, padding_length=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C\n",
    "            coordinates representing the backbone of each chain\n",
    "        target_chain_id: The chain id to sample sequences for\n",
    "        padding_length: Length of padding between concatenated chains\n",
    "    Returns:\n",
    "        Tuple (coords, seq)\n",
    "            - coords is an L x 3 x 3 array for N, CA, C coordinates, a\n",
    "              concatenation of the chains with padding in between\n",
    "            - seq is the extracted sequence, with padding tokens inserted\n",
    "              between the concatenated chains\n",
    "    \"\"\"\n",
    "    pad_coords = np.full((padding_length, 3, 3), np.nan, dtype=np.float32)\n",
    "    # For best performance, put the target chain first in concatenation.\n",
    "    coords_list = [coords[target_chain_id]]\n",
    "    for chain_id in coords:\n",
    "        if chain_id == target_chain_id:\n",
    "            continue\n",
    "        coords_list.append(pad_coords)\n",
    "        coords_list.append(coords[chain_id])\n",
    "    coords_concatenated = np.concatenate(coords_list, axis=0)\n",
    "    return coords_concatenated\n",
    "\n",
    "def sample_sequence_in_complex(model, coords, target_chain_id, temperature=1.,\n",
    "        padding_length=10, mask_pattern = None):\n",
    "    \"\"\"\n",
    "    Samples sequence for one chain in a complex.\n",
    "    Args:\n",
    "        model: An instance of the GVPTransformer model\n",
    "        coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C\n",
    "            coordinates representing the backbone of each chain\n",
    "        target_chain_id: The chain id to sample sequences for\n",
    "        padding_length: padding length in between chains\n",
    "    Returns:\n",
    "        Sampled sequence for the target chain\n",
    "    \"\"\"\n",
    "    target_chain_len = coords[target_chain_id].shape[0]\n",
    "    all_coords = _concatenate_coords(coords, target_chain_id) # puts the target chain first\n",
    "\n",
    "    # Supply padding tokens for other chains to avoid unused sampling for speed\n",
    "    padding_pattern = ['<pad>'] * all_coords.shape[0]\n",
    "    for i in range(target_chain_len):\n",
    "        padding_pattern[i] = '<mask>'\n",
    "    \n",
    "    if mask_pattern != None:\n",
    "        # make sure the supplied mask pattern is the correct length for the sequence\n",
    "\n",
    "        assert len(mask_pattern) == target_chain_len\n",
    "        for i in range(len(mask_pattern)):\n",
    "            padding_pattern[i] = mask_pattern[i]\n",
    "\n",
    "        \n",
    "    sampled = model.sample(all_coords, partial_seq=padding_pattern,\n",
    "            temperature=temperature)\n",
    "    sampled = sampled[:target_chain_len]\n",
    "    return sampled\n",
    "\n",
    "sampled_seqs = []\n",
    "for i in range(n_seqs):\n",
    "    with torch.no_grad():\n",
    "            sampled_seq = sample_sequence_in_complex(\n",
    "                model,\n",
    "                coords,\n",
    "                'C',\n",
    "                temperature = t, \n",
    "                mask_pattern = ch_c_mask\n",
    "            )\n",
    "            sampled_seqs.append(sampled_seq)\n",
    "            fout = open(pout, 'w')\n",
    "            fout.write('\\n'.join(sampled_seqs))\n",
    "            fout.close()\n",
    "            print(sampled_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea2b01-812f-4093-8634-6c42c763b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-1.9-gpu)",
   "language": "python",
   "name": "pytorch-1.9-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

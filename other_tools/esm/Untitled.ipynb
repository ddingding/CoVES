{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf335b86-5196-42af-a3fa-d4b1cc7a954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import esm\n",
    "from esm.inverse_folding import multichain_util\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "models_dir = \"models\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = float(time.time())\n",
    "print(device)\n",
    "\n",
    "\n",
    "# reading arguments\n",
    "datapath = \"/n/groups/marks/users/david/esm_if/data/gen_seqs/\"\n",
    "num_pos_mut = int(sys.argv[1])  # number of positions to mutate\n",
    "t = float(sys.argv[2])  # temperature to sample at\n",
    "n_seqs = 100  # number of sequences to sample\n",
    "\n",
    "# set the output file\n",
    "pout = datapath + \"esm_t{}_pos{}_n{}_gen_seq.csv\".format(t, num_pos_mut, n_seqs)\n",
    "print(\"writing to {}\".format(pout))\n",
    "\n",
    "\n",
    "print(\"loading model in\")\n",
    "# load model\n",
    "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "# to get rid of random dropout\n",
    "model = model.eval()\n",
    "\n",
    "print(\"reading structure in\")\n",
    "# read structure in\n",
    "cifpath = (\n",
    "    \"/n/groups/marks/users/david/esm_if/data/bio_all_rm_non_chain.cif\"\n",
    ")  # .pdb format is also acceptable\n",
    "coords, seqs = esm.inverse_folding.multichain_util.load_complex_coords(\n",
    "    cifpath, [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    ")\n",
    "################################################################################\n",
    "# masking only the mutated positions for designing sequences at these positions\n",
    "def generate_mask(wt_seq, mut_str_m1, offset=1):\n",
    "    # makes a list of positions to mask based on a mustring\n",
    "    mask_list_chC = list(wt_seq)\n",
    "\n",
    "    for m in mut_str_m1.split(\":\"):\n",
    "        wt_aa = m[0]\n",
    "        aa_pos = int(m[1:-1])\n",
    "        aa_pos_off = aa_pos - offset\n",
    "\n",
    "        assert mask_list_chC[aa_pos_off] == wt_aa  # check indexing was right\n",
    "        mask_list_chC[aa_pos_off] = \"<mask>\"\n",
    "    return mask_list_chC\n",
    "\n",
    "\n",
    "ch_c_mask_3_pos = generate_mask(seqs[\"C\"], \"D61A:K64A:E80A\", offset=2)\n",
    "ch_c_mask_4_pos = generate_mask(seqs[\"C\"], \"L59A:W60L:D61A:K64L\", offset=2)\n",
    "\n",
    "ch_c_mask_10_pos = generate_mask(\n",
    "    seqs[\"C\"], \"L48L:D52D:I53I:R55R:L56L:F74F:R78R:E80E:A81A:R82R\", offset=2\n",
    ")\n",
    "\n",
    "if num_pos_mut == 3:\n",
    "    ch_c_mask = ch_c_mask_3_pos\n",
    "elif num_pos_mut == 4:\n",
    "    ch_c_mask = ch_c_mask_4_pos\n",
    "elif num_pos_mut == 10:\n",
    "    ch_c_mask = ch_c_mask_10_pos\n",
    "else:\n",
    "    print(\"wrong input given for num positions to mutate\")\n",
    "\n",
    "\n",
    "##############################################\n",
    "##### sampling ###############################\n",
    "\n",
    "def _concatenate_coords(coords, target_chain_id, padding_length=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C\n",
    "            coordinates representing the backbone of each chain\n",
    "        target_chain_id: The chain id to sample sequences for\n",
    "        padding_length: Length of padding between concatenated chains\n",
    "    Returns:\n",
    "        Tuple (coords, seq)\n",
    "            - coords is an L x 3 x 3 array for N, CA, C coordinates, a\n",
    "              concatenation of the chains with padding in between\n",
    "            - seq is the extracted sequence, with padding tokens inserted\n",
    "              between the concatenated chains\n",
    "    \"\"\"\n",
    "    pad_coords = np.full((padding_length, 3, 3), np.nan, dtype=np.float32)\n",
    "    # For best performance, put the target chain first in concatenation.\n",
    "    coords_list = [coords[target_chain_id]]\n",
    "    for chain_id in coords:\n",
    "        if chain_id == target_chain_id:\n",
    "            continue\n",
    "        coords_list.append(pad_coords)\n",
    "        coords_list.append(coords[chain_id])\n",
    "    coords_concatenated = np.concatenate(coords_list, axis=0)\n",
    "    return coords_concatenated\n",
    "\n",
    "\n",
    "def sample_sequence_in_complex(\n",
    "    model,\n",
    "    coords,\n",
    "    target_chain_id,\n",
    "    temperature=1.0,\n",
    "    padding_length=10,\n",
    "    mask_pattern=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Samples sequence for one chain in a complex.\n",
    "    Args:\n",
    "        model: An instance of the GVPTransformer model\n",
    "        coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C\n",
    "            coordinates representing the backbone of each chain\n",
    "        target_chain_id: The chain id to sample sequences for\n",
    "        padding_length: padding length in between chains\n",
    "    Returns:\n",
    "        Sampled sequence for the target chain\n",
    "    \"\"\"\n",
    "    target_chain_len = coords[target_chain_id].shape[0]\n",
    "    all_coords = _concatenate_coords(\n",
    "        coords, target_chain_id\n",
    "    )  # puts the target chain first\n",
    "\n",
    "    # Supply padding tokens for other chains to avoid unused sampling for speed\n",
    "    padding_pattern = [\"<pad>\"] * all_coords.shape[0]\n",
    "    for i in range(target_chain_len):\n",
    "        padding_pattern[i] = \"<mask>\"\n",
    "\n",
    "    if mask_pattern != None:\n",
    "        # make sure the supplied mask pattern is the correct length for the sequence\n",
    "\n",
    "        assert len(mask_pattern) == target_chain_len\n",
    "        for i in range(len(mask_pattern)):\n",
    "            padding_pattern[i] = mask_pattern[i]\n",
    "\n",
    "    sampled = model.sample(\n",
    "        all_coords, partial_seq=padding_pattern, temperature=temperature\n",
    "    )\n",
    "    sampled = sampled[:target_chain_len]\n",
    "    return sampled\n",
    "\n",
    "\n",
    "# actual sampling\n",
    "sampled_seqs = []\n",
    "for i in range(n_seqs):\n",
    "    with torch.no_grad():\n",
    "        sampled_seq = multichain_util.sample_sequence_in_complex(\n",
    "            model, coords, \"C\", temperature=t, mask_pattern=ch_c_mask\n",
    "        )\n",
    "        sampled_seqs.append(sampled_seq)\n",
    "        # closing and opening because IO issues on cluster.\n",
    "        fout = open(pout, \"w\")\n",
    "        fout.write(\"\\n\".join(sampled_seqs))\n",
    "        fout.close()\n",
    "        print(sampled_seq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-1.9-gpu)",
   "language": "python",
   "name": "pytorch-1.9-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
